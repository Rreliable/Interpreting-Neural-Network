# Interpreting-Neural-Network

## basic

1. Reference: Zeiler, M. D., & Fergus, R. (2014). Visualizing and understanding convolutional networks. In Computer Vision–ECCV 2014 (pp. 818-833). 
2. Karen Simonyan, Andrea Vedaldi, Andrew Zisserman, “Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps”, ICLR, 2014. 
3. Integrated gradient https://arxiv.org/abs/1611.02639. 
4. DeepLIFT https://arxiv.org/abs/1704.02685. 
5. Interpretation of Neural Networks is Fragile https://arxiv.org/abs/1710.10547. 
6. Deep Neural Networks are Easily Fooled https://www.youtube.com/watch?v=M2IebCN9Ht4. 
7. With several regularization terms, and hyperparameter tuning  https://arxiv.org/abs/1506.06579    
8. Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space https://arxiv.org/abs/1612.00005. 
9. Local Interpretable ModelAgnostic Explanations (LIME). 
10. Beyond Sparsity: Tree Regularization of Deep Models for Interpretability https://arxiv.org/pdf/1711.06178.pdf     

## more
1. Network Dissection: Quantifying Interpretability of Deep Visual Representations http://netdissect.csail.mit.edu/. 
2. Grad-CAM   
3. Is attention consistent with attribution. 
4. Constraining activation map  
5. Generative CNN https://docs.google.com/presentation/d/1b0ilhvcSV44bCyj6OxkpCvQA12VrZ1FiO51GcQxvPWw/edit. 

